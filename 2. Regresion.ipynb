{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seminarios de Procesos Gaussianos\n",
    "\n",
    "### Grupo de procesamiento de la información visual (VIP) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\"> Miguel López Pérez </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Regresión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpflow\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn import model_selection\n",
    "import sklearn.datasets\n",
    "import sklearn.metrics\n",
    "import sklearn.ensemble\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(m):\n",
    "    xx = np.linspace(-0.1, 1.1, 100).reshape(100, 1)\n",
    "    mean, var = m.predict_y(xx)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(X, Y, 'kx', mew=2)\n",
    "    plt.plot(xx, mean, 'C0', lw=2)\n",
    "    plt.fill_between(xx[:,0],\n",
    "                     mean[:,0] - 2*np.sqrt(var[:,0]),\n",
    "                     mean[:,0] + 2*np.sqrt(var[:,0]),\n",
    "                     color='C0', alpha=0.2)\n",
    "    plt.xlim(-0.1, 1.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuestros datos observados van a venir de una suma de funciones trigonométricas con ruido en 12 puntos arbitrarios. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(20)\n",
    "N = 12\n",
    "X = np.random.rand(N,1)\n",
    "Y = np.sin(12*X) + 0.66*np.cos(25*X) + np.random.randn(N,1)*0.1 + 3\n",
    "\n",
    "plt.plot(X, Y, 'kx', mew=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuestro proceso gaussiano será capaz de calcular una distribución normal multivariante sobre nuestros puntos de entrenamiento. Además dado cualquier otro punto (de test) calculará la distribución normal multivariante sobre este punto de test y los de training. \n",
    "\n",
    "La varianza del likelihood sería el ruido que observa en los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = 0.1\n",
    "var = 10\n",
    "var_lik = 0.01\n",
    "k = gpflow.kernels.RBF(1, lengthscales=sc, variance = var)\n",
    "m = gpflow.models.GPR(X, Y, kern=k)\n",
    "m.likelihood.variance = var_lik\n",
    "\n",
    "plot(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pregunta**: ¿Cómo afectan los parámetros del kernel y de la verosimilitud al modelo?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimización del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Queremos encontrar los parámetros óptimos para ello los obtendremos por máxima verosimilitud. Optimizaremos los parámetros del modelo maximizando la evidencia:\n",
    "\n",
    "$\\hat{\\theta} = \\arg\\max_\\theta p(y\\mid \\theta)$ \n",
    "\n",
    "Podemos interpretar esto como que la búsqueda de los parámetros que hagan máxima la verosimilitud (\"probabilidad\") de haber observado nuestros datos.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpflow.train.ScipyOptimizer().minimize(m, maxiter=200)\n",
    "plot(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpflow.training.AdamOptimizer(0.01).minimize(m, maxiter=200)\n",
    "plot(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parámetros estimados en nuestro modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.as_pandas_table() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La incertidumbre que hemos obtenido (cuanto más grande mejor (es la log(verosimilitud))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.compute_log_likelihood()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Cuál sería la predicción en  $x = 0.5$? ¿y en $x=1$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, var = m.predict_y(np.array([0.5, 1]).reshape(2,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"El punto 0.5 tiene una distribución normal con media\", mu[0][0], \"y varianza \", var[0][0], \n",
    "      \"\\nEl punto x = 1 tiene una distribución normal con media\", mu[1][0], \"y varianza \", var[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.likelihood.variance = 0.1\n",
    "m.likelihood.variance.trainable = False\n",
    "gpflow.train.ScipyOptimizer().minimize(m, maxiter=200)\n",
    "\n",
    "plot(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.read_trainables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.compute_log_likelihood()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capacidad de generalización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Efectivamente, nuestra función predictiva se ajusta muy bien a nuestros datos observados de entrenamiento. Pero, ¿realmente está capturando la forma de la función latente? En otras palabras, ¿nuestro modelo generaliza?\n",
    "\n",
    "En esta sección, vamos a pintar en rojo la función latente de modo que vemos como de bien nuestro modelo la está reconstruyendo a partir de los datos ruidosos observados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(200)\n",
    "N = 12\n",
    "X = np.random.rand(N,1)\n",
    "Y = np.sin(12*X) + 0.66*np.cos(25*X) + np.random.randn(N,1)*0.1 + 3\n",
    "\n",
    "N = 100\n",
    "X_test = np.linspace(-0.1, 1.1, 100).reshape(100, 1)\n",
    "Y_test = np.sin(12*X_test) + 0.66*np.cos(25*X_test) + 3\n",
    "\n",
    "plt.plot(X, Y, 'kx', mew=2 , label='datos ruidosos observados')\n",
    "plt.plot(X_test, Y_test, color = 'r', lw = 0.5, label='función latente')\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = gpflow.models.GPR(X, Y, kern=k)\n",
    "m.likelihood.variance = 0.01\n",
    "gpflow.train.ScipyOptimizer().minimize(m, maxiter=200)\n",
    "plot(m)\n",
    "plt.plot(X, Y, 'kx', mew=2 , label='datos ruidosos observados')\n",
    "plt.plot(X_test, Y_test, color = 'r', lw = 0.5, label='función latente')\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.plot()\n",
    "print(m.compute_log_likelihood())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Pregunta: ** ¿Qué ocurre cuando borras la línea *m.likelihood.variance = 0.01* y vuelves a ejecutarlo? ¿Por qué?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresión lineal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = gpflow.models.GPR(X, Y, kern=gpflow.kernels.Linear(1))\n",
    "m.likelihood.variance = 0.01\n",
    "gpflow.train.ScipyOptimizer().minimize(m, maxiter=200)\n",
    "plot(m)\n",
    "plt.plot(X, Y, 'kx', mew=2 , label='datos ruidosos observados')\n",
    "plt.plot(X_test, Y_test, color = 'r', lw = 0.5, label='función latente')\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pregunta:** ¿Qué obtendríamos con el kernel Matern12?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo con el data set de *Diabetes* de sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = sklearn.datasets.load_diabetes(return_X_y=True)\n",
    "print('El tamaño de este dataset es', X.shape)\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.4, random_state = 100)\n",
    "m = gpflow.models.GPR(X_train, y_train.reshape(-1,1), kern=gpflow.kernels.RBF(X_train.shape[1]) + gpflow.kernels.White(X_train.shape[1], 1e-5))\n",
    "m.likelihood.variance = 0.1\n",
    "gpflow.train.ScipyOptimizer().minimize(m, maxiter=500)\n",
    "print('La log-verosimilitud del modelo es', m.compute_log_likelihood())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_gp, var = m.predict_y(X_test)\n",
    "mse_gp = sklearn.metrics.mean_squared_error(y_test, pred_gp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = sklearn.ensemble.RandomForestRegressor(n_estimators = 200, max_depth = 12, random_state = 12)\n",
    "rf_model.fit(X_train, y_train)\n",
    "pred_rf = rf_model.predict(X_test)\n",
    "mse_rf = sklearn.metrics.mean_squared_error(y_test, pred_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('El mse del random forest es ', mse_rf, 'mientras que en el modelo GP es ', mse_gp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
