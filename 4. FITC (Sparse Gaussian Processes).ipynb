{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seminarios de Procesos Gaussianos\n",
    "\n",
    "### Grupo de procesamiento de la información visual (VIP) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\"> Miguel López Pérez - mlopezp@ugr.es </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GUIÓN IV: PUNTOS INDUCTORES Y FITC\n",
    "\n",
    "En este guión vamos a estudiar como los puntos inductores nos ayudarán a resolver los problemas asociados a la estimación de parámetros y predicción cuando tenemos muchos datos. Veremos una aproximación basada en la aproximación de la distribución a priori (el GP). En sesiones posteriores estudiaremos otros métodos que se basan en la aproximación de la distribución a posteriori del GP dadas las observaciones. Comenzados importando lo que necesitamos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpflow\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster.vq import kmeans2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En primer lugar vamos a definir unas funciones auxiliares que pintarán nuestros modelos. \n",
    "  \n",
    "  - *plot* pintará los datos ruidosos observados junto a nuestro proceso Gaussiano. \n",
    "  - *plot_Z* pintará los datos ruidosos observados, el proceso Gaussiano y las entradas inductoras estimadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(m):\n",
    "    xx = np.linspace(-0.1, 1.1, 100).reshape(100, 1)\n",
    "    mean, var = m.predict_y(xx)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(X, Y, 'kx', mew=2, label='datos ruidosos observados')\n",
    "    plt.plot(xx, mean, 'C0', lw=2)\n",
    "    plt.fill_between(xx[:,0],\n",
    "                     mean[:,0] - 2*np.sqrt(var[:,0]),\n",
    "                     mean[:,0] + 2*np.sqrt(var[:,0]),\n",
    "                     color='C0', alpha=0.2)\n",
    "    plt.xlim(-0.1, 1.1)\n",
    "    \n",
    "def plot_Z(m):\n",
    "    xx = np.linspace(-0.1, 1.1, 100).reshape(100, 1)\n",
    "    mean, var = m.predict_y(xx)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(X, Y, 'kx', mew=2, label='datos ruidosos observados')\n",
    "    plt.plot(xx, mean, 'C0', lw=2)\n",
    "    Z = m.feature.Z.read_value()\n",
    "    plt.plot(Z, np.zeros(len(Z)), '*', mew=2, color = 'red', label='entradas inductoras estimadas')\n",
    "    plt.fill_between(xx[:,0],\n",
    "                     mean[:,0] - 2*np.sqrt(var[:,0]),\n",
    "                     mean[:,0] + 2*np.sqrt(var[:,0]),\n",
    "                     color='C0', alpha=0.2)\n",
    "    plt.xlim(-0.1, 1.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully Independent Training Conditional (FITC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando tenemos muchos datos los gpS sufren problemas debido al hecho de tener que invertir una matriz demasiado grande. Para resolver esto, están los llamados modelos Sparse que introducen inducing points (puntos inductores). De este modo, la matriz de covarianza tan grande solo deberá ser evaluada en los inducing points reduciendo su tamaño (y, por tanto, facilitando su inversión). \n",
    "\n",
    "La primera aproximación es la FITC, en esta aproximación consideramos que $f$ y $f_*$ son condicionalmente independientes dados los inducing points. Es decir, aproximamos $$p(f, f_*) \\sim q(f,f_*) = \\int q(f_*\\mid u) q(f\\mid u) p(u) du$$\n",
    "\n",
    "Mira la presentación para entender como se realiza el aprendizaje y la inferencia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a ver un ejemplo en 1D sencillo para entender el modelo FITC. Para ello, consideramos nuestra función latente $f$:\n",
    "\n",
    "\n",
    "$$f(x)= 5\\exp\\left(-35(x-0.1)^2\\right) + 4\\exp\\left(-40(x-0.9)^2\\right)$$\n",
    "\n",
    "\n",
    "Véase que son la suma de dos funciones gaussianas. Consideramos $N=12$ observaciones ruidosas: (con ruido gaussiano aditivo) $\\{(x_i, y_i)\\},\\quad i=1,\\dots,N$ \n",
    "\n",
    "$$y_i = f(x_i) + \\epsilon, \\quad \\epsilon_i\\sim\\mathcal{N}(0,0.01), \\qquad i=1,\\dots,N$$,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(200) #Semilla para que sea reproducible\n",
    "N = 12 #Número de puntos observados\n",
    "X = np.random.rand(N,1) #Rasgos observados\n",
    "Y = 5 * np.exp(-35*(X-0.1)**2) + 4* np.exp(-40*(X-0.9)**2) + np.random.randn(N,1)*0.1 #Variable objetivo observada\n",
    "\n",
    "#Función latente\n",
    "N = 100\n",
    "X_real = np.linspace(-0.1, 1.1, 100).reshape(100, 1)\n",
    "Y_real = 5 * np.exp(-35*(X_real-0.1)**2) + 4* np.exp(-40*(X_real-0.9)**2)\n",
    "\n",
    "#Pintamos los datos\n",
    "plt.plot(X, Y, 'kx', mew=2, label='datos ruidosos observados') \n",
    "plt.plot(X_real, Y_real, color = 'r', lw = 0.5, label='función latente')\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GP para regresión"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observa que en un probelam real, tú solo tendrás acceso a los datos ruidosos y obviamente no a la función latente.\n",
    "\n",
    "En la siguiente celda tenemos cual sería el modelo ajustado usando todos los datos observados. También se muestra la log verosimilitud del modelo. Recordamos que la log verosimilitud del modelo es $ \\log p(y\\mid x, \\theta)$. Cuanto más alto sea más verosímiles son los datos ruidosos observados dados los rasgos y los parámetros estimados. Recordamos también que está es la función objetivo a maximizar cuando hallamos los parámetros del modelo.\n",
    "\n",
    "Ajustamos un modelo de regresión usando un kernel RBF y usando el optimizador Adam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = 0.1\n",
    "var = 10\n",
    "var_lik = 0.01\n",
    "\n",
    "k = gpflow.kernels.RBF(1, lengthscales=sc, variance = var)\n",
    "m = gpflow.models.GPR(X, Y, kern=k)\n",
    "m.likelihood.variance = var_lik\n",
    "gpflow.training.AdamOptimizer(0.01).minimize(m, maxiter = 200)\n",
    "\n",
    "plot(m)\n",
    "plt.plot(X_real, Y_real, color = 'r', lw = 0.5, label='función latente')\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.plot()\n",
    "print('La log-verosimilitud del modelo es', m.compute_log_likelihood())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recuerda lo que significan las franjas azules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aproximación para regresión usando GP: FITC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la siguiente celda, vamos a aplicar el modelo FITC. Este modelo usa puntos inductores, llamaremos entradas (o localizaciones) inductoras a los $Z$ y salidas inductoras $u = f(z)$ el valor de la función latente en las respectivas entradas inductoras. Esto puede liarte un poco. Con el tiempo para $u$ calcularemos una aproximación de una distribución a posteriori pero de momento nos quedamos en el GP original.\n",
    "\n",
    "Elegiremos un número de puntos inductores $M$.Podemos alterar el número $M$ de puntos inductores y ver que modelos salen con su respectiva log-verosimilitud. Las entradas inductoras se iniciarán usando un algoritmo kmeans ($M$ clusters) sobre los datos, donde nos quedaremos con los respectivos centroides. Al entrenar el modelo, obtendremos las localizaciones de las entradas inductoras así como los parámetros del modelo. Con todo ello, realizaremos las predicciones (mira la presentación de esta sesión)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 5 #Numero de inducing points\n",
    "sc = 0.1\n",
    "var = 10\n",
    "var_lik = 0.01\n",
    "\n",
    "Z = kmeans2(X, M, minit='points')[0]\n",
    "k = gpflow.kernels.RBF(1, lengthscales=sc, variance = var)\n",
    "m = gpflow.models.GPRFITC(X, Y, kern=k, Z = Z) #modelo FITC\n",
    "m.likelihood.variance = var_lik\n",
    "gpflow.training.AdamOptimizer(0.01).minimize(m, maxiter = 200)\n",
    "\n",
    "plot_Z(m)\n",
    "plt.plot(Z, -1 + np.zeros(len(Z)), '^', mew=2, color = 'blue', label='entradas inductoras iniciales')\n",
    "plt.plot(X_real, Y_real, color = 'r', lw = 0.5, label='función latente')\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.plot()\n",
    "print('La log-verosimilitud del modelo es', m.compute_log_likelihood())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observa esta gráfica, es muy interesante. Hemos dibujado las entradas inductoras. ¿Por qué no hemos dibujado las salidas?. Observa la parte de la izquierda de la gráfica, ¿qué crees que está pasando?.\n",
    "\n",
    "En la siguiente celda podemos elegir donde colocar las entradas inductoras y ver el GP resultante usando FITC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = [[0.01], [0.11], [0.24], [0.9], [1]] # Inducing points\n",
    "m = gpflow.models.SGPR(X, Y, kern=k, Z = Z)\n",
    "m.feature.set_trainable(False)\n",
    "plot_Z(m)\n",
    "plt.plot(X_real, Y_real, color = 'r', lw = 0.5, label='función latente')\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "print('La log-verosimilitud del modelo es', m.compute_log_likelihood())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aplicación: denoising o eliminación de ruido de imágenes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ilustrar la necesidad de utilizar aproximaciones cuando tenemos un elevado número de datos,  vamos a elegir el problema de denoising de imágenes. Este es un problema inverso (mal planteado) en el campo de restauración de imágenes. El problema consiste en  recuperar la imagen latente sin ruido de una imagen observada con cierto ruido (gaussiano).\n",
    "\n",
    "Vamos a generar una imagen sintética en escala de grises. A continuación le añadiremos ruido gaussiano.  \n",
    "\n",
    "La función latente que usaremos será $f$:\n",
    "\n",
    "$$f(x_1, x_2) = x_1(1-x_1)\\cos(4\\pi x_1)\\sin(4\\pi x_2^2)^2$$\n",
    "\n",
    "Esta función la observaremos en un grid del cuadrado $[0,1]\\times[0,1]$, exactamente haremos un grid con $300\\times300=90.000$ puntos equidistribuidos. Al valor resultante le añadiremos ruido gaussiano:\n",
    "\n",
    "$$\\epsilon \\sim\\mathcal(0, 0.08^2)$$\n",
    "\n",
    "Nuestro problema sería entonces en conseguir recuperar la función latente $f$ que no tiene ruido:\n",
    "\n",
    "$$y = f(x_1,x_2) + \\epsilon$$\n",
    "\n",
    "Nótese: que al hacer regresión con los procesos gaussianos queremos modelizar la función latente, no queremos modelizar la variable ruidosa observada. Por tanto, los procesos gaussianos nos valen para hacer denoising."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definimos la función latente y el grid donde la evaluaremos\n",
    "def func(x, y):\n",
    "     return x*(1-x)*np.cos(4*np.pi*x) * np.sin(4*np.pi*y**2)**2\n",
    "grid_x, grid_y = np.mgrid[0:1:300j, 0:1:300j]\n",
    "\n",
    "#Generamos la imagen A\n",
    "A = func(grid_x, grid_y)\n",
    "#Creamos la imagen ruidosa de A\n",
    "ruido = np.random.normal(0, 0.08, A.shape)\n",
    "A_ruidosa = A + ruido"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostramos la imagen latente y la imagen ruidosa observada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(6,6))\n",
    "ax = axes.ravel()\n",
    "\n",
    "ax[0].imshow(A, cmap='gray')\n",
    "ax[0].set_title('A')\n",
    "ax[0].set_axis_off()\n",
    "\n",
    "ax[1].imshow(A_ruidosa,\n",
    "             extent=(0,1,0,1),\n",
    "             cmap='gray')\n",
    "ax[1].set_title('A_ruidosa')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generamos nuestro dataset, tendremos $2$ rasgos $(x_1, x_2)$ por cada instancia, los rasgos serán la posición espacial, y una variable ruidosa observada $y$. Nuestro dataset tendría la siguiente forma:\n",
    "\n",
    "\n",
    "|   |   $x_1$  |   $x_2$  |     y    |\n",
    "|:-:|:--------:|:--------:|:--------:|\n",
    "|   |     0    |     0    |  0.05585 |\n",
    "|   |     0    |  0.00334 |  0.00425 |\n",
    "|   |     0    |  0.00668 |  0.65891 |\n",
    "|   | $\\vdots$ | $\\vdots$ | $\\vdots$ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construimos un \"dataset\" que contendrá las coordenadas de cada punto\n",
    "from itertools import product\n",
    "l = np.array(list(product(np.linspace(0,1,300), np.linspace(0,1,300))))\n",
    "\n",
    "#La variable objetivo serán los valores observados (A ruidosa)\n",
    "Y = A_ruidosa.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para este problema no podemos usar los GP ya que el tamaño de la imagen es $300\\times 300$ tenemos $90.000$ datos.\n",
    "\n",
    "Nos devuelve el error: *ResourceExhaustedError: OOM when allocating tensor with shape[90000,90000] *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = 0.1\n",
    "var = 1\n",
    "var_lik = 0.08\n",
    "k = gpflow.kernels.RBF(2, lengthscales=sc, variance = var)\n",
    "m = gpflow.models.GPR(l, Y, kern=k)\n",
    "\n",
    "#Optimización del modelo\n",
    "gpflow.training.AdamOptimizer(0.01).minimize(m, maxiter=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a usar la aproximación FITC con 100 puntos inductores usando dos kernels distintos: RBF y Matern32. Vamos primero a realizar las estimaciones e inferencias correspondientes y luego dibujaremos todos los resultados. Va a tardar un poco, sería bueno que mientras tanto pensases lo que va a salir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Kernel: RBF **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modelo con 100 inducing points\n",
    "M = 100\n",
    "Z = kmeans2(l, M, minit='points')[0]\n",
    "\n",
    "sc = 0.1\n",
    "var = 1\n",
    "var_lik = 0.08\n",
    "k = gpflow.kernels.RBF(2, lengthscales=sc, variance = var)\n",
    "m = gpflow.models.GPRFITC(l, A_ruidosa.reshape(-1,1), kern=k, Z=Z)\n",
    "\n",
    "#Optimización del modelo\n",
    "gpflow.training.AdamOptimizer(0.01).minimize(m, maxiter=100)\n",
    "\n",
    "#Predicción\n",
    "Y_pred, var = m.predict_y(l)\n",
    "Y_RBF = Y_pred.reshape(300,300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Kernel: Matern32 **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modelo con 100 inducing points\n",
    "M = 100\n",
    "Z = kmeans2(l, M, minit='points')[0]\n",
    "\n",
    "sc = 0.1\n",
    "var = 1\n",
    "var_lik = 0.08\n",
    "k = gpflow.kernels.Matern32(2, lengthscales=sc, variance = var)\n",
    "m1 = gpflow.models.GPRFITC(l, A_ruidosa.reshape(-1,1), kern=k, Z=Z)\n",
    "\n",
    "#Optimización del modelo\n",
    "gpflow.training.AdamOptimizer(0.01).minimize(m1, maxiter=100)\n",
    "\n",
    "#Predicción\n",
    "Y_pred, var = m1.predict_y(l)\n",
    "Y_Matern = Y_pred.reshape(300,300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(8,8))\n",
    "ax = axes.ravel()\n",
    "\n",
    "ax[0].imshow(A, cmap='gray',\n",
    "             extent=(0,1,0,1))\n",
    "ax[0].set_title('A')\n",
    "ax[0].set_axis_off()\n",
    "\n",
    "ax[1].imshow(A_ruidosa,\n",
    "             extent=(0,1,0,1),\n",
    "             cmap='gray')\n",
    "ax[1].set_title('A - ruidosa')\n",
    "\n",
    "ax[2].imshow(Y_RBF, cmap='gray',\n",
    "             extent=(0,1,0,1))\n",
    "ax[2].set_title('A - RBF')\n",
    "\n",
    "ax[3].imshow(Y_Matern,\n",
    "             extent=(0,1,0,1),\n",
    "             cmap='gray')\n",
    "ax[3].set_title('A - Matern')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último, antes de dibujar todo, vamos a incluir también las posiciones de las entradas inductoras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(8,8))\n",
    "ax = axes.ravel()\n",
    "\n",
    "ax[0].imshow(A, cmap='gray',\n",
    "             extent=(0,1,0,1))\n",
    "ax[0].set_title('A')\n",
    "ax[0].set_axis_off()\n",
    "\n",
    "ax[1].imshow(A_ruidosa,\n",
    "             extent=(0,1,0,1),\n",
    "             cmap='gray')\n",
    "ax[1].set_title('A - ruidosa')\n",
    "\n",
    "ax[2].imshow(Y_RBF, cmap='gray',\n",
    "             extent=(0,1,0,1))\n",
    "ax[2].set_title('A - RBF')\n",
    "\n",
    "\n",
    "ax[3].imshow(Y_Matern,\n",
    "             extent=(0,1,0,1),\n",
    "             cmap='gray')\n",
    "ax[3].set_title('A - Matern')\n",
    "\n",
    "\n",
    "Z = m.feature.Z.read_value()\n",
    "Z_1 = [x[0] for x in Z]\n",
    "Z_2 = [x[1] for x in Z]\n",
    "ax[2].plot(Z_1, Z_2, 'o', label = 'entradas inductoras estimadas')\n",
    "\n",
    "Z = m1.feature.Z.read_value()\n",
    "Z_1 = [x[0] for x in Z]\n",
    "Z_2 = [x[1] for x in Z]\n",
    "ax[3].plot(Z_1, Z_2, 'o', label = 'entradas inductoras estimadas')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Qué pasaría si usaramos una escala más pequeña en el núcleo RBF?. Piensa la respuesta antes de ejecutar las celdas siguientes. Comenza Los valores estimados del kernel son los siguientes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.kern.as_pandas_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejecutamos el modelo RBF con distintas escalas entre 0.15 y 0.01 y las pintamos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modelo con 100 inducing points\n",
    "M = 100\n",
    "Z = kmeans2(l, M, minit='points')[0]\n",
    "\n",
    "sc = np.linspace(0.15, 0.01, 10)\n",
    "Y_RBF_list = []\n",
    "\n",
    "for sc_i in sc:\n",
    "    var = 0.71\n",
    "    var_lik = 0.08\n",
    "    k = gpflow.kernels.RBF(2, lengthscales=sc_i, variance = var)\n",
    "    m = gpflow.models.GPRFITC(l, A_ruidosa.reshape(-1,1), kern=k, Z=Z)\n",
    "\n",
    "    #Predicción\n",
    "    Y_pred, var = m.predict_y(l)\n",
    "    Y_RBF = Y_pred.reshape(300,300)\n",
    "    Y_RBF_list.append(Y_RBF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(4, 3, figsize=(14,14))\n",
    "ax = axes.ravel()\n",
    "\n",
    "ax[0].imshow(A, cmap='gray',\n",
    "             extent=(0,1,0,1))\n",
    "ax[0].set_title('A')\n",
    "ax[0].set_axis_off()\n",
    "\n",
    "ax[1].imshow(A_ruidosa,\n",
    "             extent=(0,1,0,1),\n",
    "             cmap='gray')\n",
    "ax[1].set_title('A - ruidosa')\n",
    "\n",
    "for i in range(2,12):\n",
    "    ax[i].imshow(Y_RBF_list[i-2], cmap='gray',\n",
    "                 extent=(0,1,0,1))\n",
    "    ax[i].set_title('A - RBF, sc = {0:.2f}'.format(sc[i-2]))\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Y si aumentamos la escala?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
